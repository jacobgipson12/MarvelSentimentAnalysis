{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e20d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ee094",
   "metadata": {},
   "source": [
    "## First compile the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37cc6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.imdb.com/title/tt10954600/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt9114286/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt10648342/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt9419884/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt10872600/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt9032400/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt9376612/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt3480822/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt6320628/reviews?ref_=tt_urv',\n",
    "        'https://www.imdb.com/title/tt4154796/reviews?ref_=tt_urv'\n",
    "       ]\n",
    "# Antman and the Wasp: Quantumania\n",
    "# Black Panther: Wakanda Forever\n",
    "# Thor Love and Thunder\n",
    "# Doctor Strange in the Multiverse of Madness\n",
    "# Spider-Man: No Way HOme\n",
    "# Eternals\n",
    "# Shang-Chi and the leend of the Ten Rings\n",
    "# Black Widow\n",
    "# Spider Man Far From Home\n",
    "# Avengers Endgame\n",
    "\n",
    "directory = []\n",
    "for i in urls:\n",
    "    req = Request(i)\n",
    "    html_page = urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "    all_links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        all_links.append(link.get('href'))\n",
    "\n",
    "    review_links = []\n",
    "    random.shuffle(all_links)\n",
    "    for i in all_links:\n",
    "        if len(review_links) == 10:\n",
    "            break\n",
    "        if 'review/' in i:\n",
    "            review_links.append(i)\n",
    "    for i in review_links:\n",
    "        directory.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047b5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/review/rw8884834/', '/review/rw8875497/', '/review/rw8873784/', '/review/rw8877495/', '/review/rw8879707/', '/review/rw8875320/', '/review/rw8884834/', '/review/rw9002216/', '/review/rw8885390/', '/review/rw8879122/', '/review/rw8844815/', '/review/rw8695632/', '/review/rw8679686/', '/review/rw8683831/', '/review/rw8668856/', '/review/rw8668856/', '/review/rw8670726/', '/review/rw8718626/', '/review/rw8668496/', '/review/rw8664025/', '/review/rw8332897/', '/review/rw8329077/', '/review/rw8315692/', '/review/rw8306334/', '/review/rw8304194/', '/review/rw8331400/', '/review/rw8351608/', '/review/rw8311274/', '/review/rw8524888/', '/review/rw8309971/', '/review/rw8361636/', '/review/rw8117112/', '/review/rw8114393/', '/review/rw8118284/', '/review/rw8143965/', '/review/rw8114820/', '/review/rw8125279/', '/review/rw8114376/', '/review/rw8120384/', '/review/rw8114393/', '/review/rw8009900/', '/review/rw7665817/', '/review/rw7651439/', '/review/rw7648266/', '/review/rw7649796/', '/review/rw8021381/', '/review/rw7646956/', '/review/rw7946288/', '/review/rw7728647/', '/review/rw7984992/', '/review/rw8549850/', '/review/rw7764128/', '/review/rw7511155/', '/review/rw7760759/', '/review/rw7593415/', '/review/rw7616475/', '/review/rw7758352/', '/review/rw7762205/', '/review/rw7511155/', '/review/rw8549850/', '/review/rw7911359/', '/review/rw7585057/', '/review/rw7569557/', '/review/rw7364853/', '/review/rw7364853/', '/review/rw7345237/', '/review/rw7620777/', '/review/rw7588221/', '/review/rw7322425/', '/review/rw7537673/', '/review/rw9065253/', '/review/rw8969918/', '/review/rw9133292/', '/review/rw8962996/', '/review/rw7114474/', '/review/rw7115310/', '/review/rw8707318/', '/review/rw9133292/', '/review/rw7138947/', '/review/rw7104179/', '/review/rw5009648/', '/review/rw4988554/', '/review/rw5005207/', '/review/rw5009648/', '/review/rw5632267/', '/review/rw4985743/', '/review/rw5028646/', '/review/rw4988334/', '/review/rw4981921/', '/review/rw4985597/', '/review/rw7585695/', '/review/rw4801247/', '/review/rw4807795/', '/review/rw5069780/', '/review/rw4826082/', '/review/rw4802892/', '/review/rw4799176/', '/review/rw4837185/', '/review/rw5044156/', '/review/rw4807795/']\n"
     ]
    }
   ],
   "source": [
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f6a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://imdb.com/review/rw8884834/', 'https://imdb.com/review/rw8875497/', 'https://imdb.com/review/rw8873784/', 'https://imdb.com/review/rw8877495/', 'https://imdb.com/review/rw8879707/', 'https://imdb.com/review/rw8875320/', 'https://imdb.com/review/rw8884834/', 'https://imdb.com/review/rw9002216/', 'https://imdb.com/review/rw8885390/', 'https://imdb.com/review/rw8879122/', 'https://imdb.com/review/rw8844815/', 'https://imdb.com/review/rw8695632/', 'https://imdb.com/review/rw8679686/', 'https://imdb.com/review/rw8683831/', 'https://imdb.com/review/rw8668856/', 'https://imdb.com/review/rw8668856/', 'https://imdb.com/review/rw8670726/', 'https://imdb.com/review/rw8718626/', 'https://imdb.com/review/rw8668496/', 'https://imdb.com/review/rw8664025/', 'https://imdb.com/review/rw8332897/', 'https://imdb.com/review/rw8329077/', 'https://imdb.com/review/rw8315692/', 'https://imdb.com/review/rw8306334/', 'https://imdb.com/review/rw8304194/', 'https://imdb.com/review/rw8331400/', 'https://imdb.com/review/rw8351608/', 'https://imdb.com/review/rw8311274/', 'https://imdb.com/review/rw8524888/', 'https://imdb.com/review/rw8309971/', 'https://imdb.com/review/rw8361636/', 'https://imdb.com/review/rw8117112/', 'https://imdb.com/review/rw8114393/', 'https://imdb.com/review/rw8118284/', 'https://imdb.com/review/rw8143965/', 'https://imdb.com/review/rw8114820/', 'https://imdb.com/review/rw8125279/', 'https://imdb.com/review/rw8114376/', 'https://imdb.com/review/rw8120384/', 'https://imdb.com/review/rw8114393/', 'https://imdb.com/review/rw8009900/', 'https://imdb.com/review/rw7665817/', 'https://imdb.com/review/rw7651439/', 'https://imdb.com/review/rw7648266/', 'https://imdb.com/review/rw7649796/', 'https://imdb.com/review/rw8021381/', 'https://imdb.com/review/rw7646956/', 'https://imdb.com/review/rw7946288/', 'https://imdb.com/review/rw7728647/', 'https://imdb.com/review/rw7984992/', 'https://imdb.com/review/rw8549850/', 'https://imdb.com/review/rw7764128/', 'https://imdb.com/review/rw7511155/', 'https://imdb.com/review/rw7760759/', 'https://imdb.com/review/rw7593415/', 'https://imdb.com/review/rw7616475/', 'https://imdb.com/review/rw7758352/', 'https://imdb.com/review/rw7762205/', 'https://imdb.com/review/rw7511155/', 'https://imdb.com/review/rw8549850/', 'https://imdb.com/review/rw7911359/', 'https://imdb.com/review/rw7585057/', 'https://imdb.com/review/rw7569557/', 'https://imdb.com/review/rw7364853/', 'https://imdb.com/review/rw7364853/', 'https://imdb.com/review/rw7345237/', 'https://imdb.com/review/rw7620777/', 'https://imdb.com/review/rw7588221/', 'https://imdb.com/review/rw7322425/', 'https://imdb.com/review/rw7537673/', 'https://imdb.com/review/rw9065253/', 'https://imdb.com/review/rw8969918/', 'https://imdb.com/review/rw9133292/', 'https://imdb.com/review/rw8962996/', 'https://imdb.com/review/rw7114474/', 'https://imdb.com/review/rw7115310/', 'https://imdb.com/review/rw8707318/', 'https://imdb.com/review/rw9133292/', 'https://imdb.com/review/rw7138947/', 'https://imdb.com/review/rw7104179/', 'https://imdb.com/review/rw5009648/', 'https://imdb.com/review/rw4988554/', 'https://imdb.com/review/rw5005207/', 'https://imdb.com/review/rw5009648/', 'https://imdb.com/review/rw5632267/', 'https://imdb.com/review/rw4985743/', 'https://imdb.com/review/rw5028646/', 'https://imdb.com/review/rw4988334/', 'https://imdb.com/review/rw4981921/', 'https://imdb.com/review/rw4985597/', 'https://imdb.com/review/rw7585695/', 'https://imdb.com/review/rw4801247/', 'https://imdb.com/review/rw4807795/', 'https://imdb.com/review/rw5069780/', 'https://imdb.com/review/rw4826082/', 'https://imdb.com/review/rw4802892/', 'https://imdb.com/review/rw4799176/', 'https://imdb.com/review/rw4837185/', 'https://imdb.com/review/rw5044156/', 'https://imdb.com/review/rw4807795/']\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "for i in directory:\n",
    "    links.append('https://imdb.com'+i)\n",
    "    \n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194151ad",
   "metadata": {},
   "source": [
    "I compiled a list of reviews for various Marvel movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc1a022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = []\n",
    "for i in links:\n",
    "    url = i\n",
    "    res = requests.get(url)\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    reviews.append(soup.select(\".text\")[0].text)\n",
    "\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01fd26aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........and that literally is a line from the film, summing up how the MCU has become since Covid hit the world. Disney are now jumping the shark 31 films into the universe......Scott Lang and Hope Van Dyne return to continue their adventures as Ant-Man and The Wasp. Together, with Hope's parents Janet and Hank Pym, and Scott's daughter Cassie Lang, the family finds themselves exploring the pretty redundant Quantum Realm, interacting with strange new CGI creatures, and monstrosities, and embarking on an adventure that will push them beyond the limits of boredom, and what they thought possible.I love Ant-Man. The first two films were so so different to anything that Marvel had released, and Paul Rudd was amazing as the titular hero. It has humour that no other Marvel humour had, it was childish, and it was all the better for it. It dared to be different, and the payoff was something fresh, even when the marvel behemoth was becoming a little tired, especially after the debacle that was Age Of Ultron. If Wright wasn't given the cold shoulder, we could be watching something different, and while i don't think the blame should be placed on Reed's shoulders, the studio had the final say, it just doesn't have the panache that the first film had.It's boring, and dull, and the effects all meld into one after a while. It's like the makers have taken the cantina scene from A New Hope, and decided to make a whole movie surrounding it, adding a supposed bigger adversary than Thaos. Kang is not that villain. Majors seems to be homing his most campest bone in his body, as there is nothing remotely sinister about his character, he just tells people to shut up, and make their costumes slightly tighter. Add a worthless cameo from Murray, and what on earth did they do with M. O. D. O. K? And you have a Marvel film that lays in the bowels on the MCU along with Eternals, Love and Thunder, and The Dark World.Douglas and Pfeiffer are unforgivably wasted in this, Douglas there because of something that happens at the end with the ants, and Pfeieffer just there to tell the story of Kang. Little support is offered, aside from a guy who can read minds who's one running joke wears really thin, and a Valkyries wannabe. This was the franchise in the MCU that i wanted to succeed, because Rudd is just so brilliant and funny in his role. Here, he is just a one dimensional hero who multiplies to become even more unfunny versions of himself.It's a real shame, really a shame, but hey, the MCU has has the Taika touch, and we all now he really isn't that good.I have really low expectations now for the rest of phase 5.And 6, and 7.This will go on forever.......Help us.\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd5d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               corpus\n",
      "0   .........and that literally is a line from the...\n",
      "1   Well, I'll start off by saying that this wasn'...\n",
      "2   \"Ant-Man and the Wasp: Quantumania\" is the fir...\n",
      "3   So, I'm not going to say this is a great Marve...\n",
      "4   We go down into the quantum world and enter pl...\n",
      "..                                                ...\n",
      "95  After Avengers Infinity War, we waited for the...\n",
      "96  If you're going to watch this movie, avoid any...\n",
      "97  After watching Infinity war, I was looking for...\n",
      "98  I had to take several breaks, walk the dogs, p...\n",
      "99  So it all ends. I have loved most of the MCU, ...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(reviews, columns=[\"corpus\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bd5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    # remove links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove special chars and numbers\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        # 1. tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # 2. check if stopword\n",
    "        tokens = [w for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
    "        # 3. join back together\n",
    "        text = \" \".join(tokens)\n",
    "    # return text in lower case and stripped of whitespaces\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def0e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['corpus'].apply(lambda x: preprocess_text(x, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cef173a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'literally line film summing mcu become since covid hit world disney jumping shark films universe scott lang hope van dyne return continue adventures ant man wasp together hope parents janet hank pym scott daughter cassie lang family finds exploring pretty redundant quantum realm interacting strange new cgi creatures monstrosities embarking adventure push beyond limits boredom thought possible love ant man first two films different anything marvel released paul rudd amazing titular hero humour marvel humour childish better dared different payoff something fresh even marvel behemoth becoming little tired especially debacle age ultron wright given cold shoulder could watching something different think blame placed reed shoulders studio final say panache first film boring dull effects meld one like makers taken cantina scene new hope decided make whole movie surrounding adding supposed bigger adversary thaos kang villain majors seems homing campest bone body nothing remotely sinister character tells people shut make costumes slightly tighter add worthless cameo murray earth k marvel film lays bowels mcu along eternals love thunder dark world douglas pfeiffer unforgivably wasted douglas something happens end ants pfeieffer tell story kang little support offered aside guy read minds one running joke wears really thin valkyries wannabe franchise mcu wanted succeed rudd brilliant funny role one dimensional hero multiplies become even unfunny versions real shame really shame hey mcu taika touch really good really low expectations rest phase go forever help us'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269d94a",
   "metadata": {},
   "source": [
    "## Load one of the sentiment vocabularies and run the sentiment analyzer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c63a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "134ea8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.........and that literally is a line from the...</td>\n",
       "      <td>literally line film summing mcu become since c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, I'll start off by saying that this wasn'...</td>\n",
       "      <td>well start saying bad movie great alright like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Ant-Man and the Wasp: Quantumania\" is the fir...</td>\n",
       "      <td>ant man wasp quantumania first film phase marv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So, I'm not going to say this is a great Marve...</td>\n",
       "      <td>going say great marvel film however also say b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We go down into the quantum world and enter pl...</td>\n",
       "      <td>go quantum world enter planet cgi apart actors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>After Avengers Infinity War, we waited for the...</td>\n",
       "      <td>avengers infinity war waited avengers endgame ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>If you're going to watch this movie, avoid any...</td>\n",
       "      <td>going watch movie avoid spoilers even spoiler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>After watching Infinity war, I was looking for...</td>\n",
       "      <td>watching infinity war looking forward much tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I had to take several breaks, walk the dogs, p...</td>\n",
       "      <td>take several breaks walk dogs play videogames ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>So it all ends. I have loved most of the MCU, ...</td>\n",
       "      <td>ends loved mcu predictable awaited iw see woul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               corpus  \\\n",
       "0   .........and that literally is a line from the...   \n",
       "1   Well, I'll start off by saying that this wasn'...   \n",
       "2   \"Ant-Man and the Wasp: Quantumania\" is the fir...   \n",
       "3   So, I'm not going to say this is a great Marve...   \n",
       "4   We go down into the quantum world and enter pl...   \n",
       "..                                                ...   \n",
       "95  After Avengers Infinity War, we waited for the...   \n",
       "96  If you're going to watch this movie, avoid any...   \n",
       "97  After watching Infinity war, I was looking for...   \n",
       "98  I had to take several breaks, walk the dogs, p...   \n",
       "99  So it all ends. I have loved most of the MCU, ...   \n",
       "\n",
       "                                              cleaned  \n",
       "0   literally line film summing mcu become since c...  \n",
       "1   well start saying bad movie great alright like...  \n",
       "2   ant man wasp quantumania first film phase marv...  \n",
       "3   going say great marvel film however also say b...  \n",
       "4   go quantum world enter planet cgi apart actors...  \n",
       "..                                                ...  \n",
       "95  avengers infinity war waited avengers endgame ...  \n",
       "96  going watch movie avoid spoilers even spoiler ...  \n",
       "97  watching infinity war looking forward much tim...  \n",
       "98  take several breaks walk dogs play videogames ...  \n",
       "99  ends loved mcu predictable awaited iw see woul...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8861d0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.118, 'neu': 0.61, 'pos': 0.272, 'compound': 0.9936}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores(df['cleaned'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f26c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "for i in df['cleaned']:\n",
    "    sentiments.append(sentiment.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5831756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sentiments = []\n",
    "for i in sentiments:\n",
    "    if i['compound'] < -0.2:\n",
    "        gen_sentiments.append('negative')\n",
    "    elif i['compound'] > 0.2:\n",
    "        gen_sentiments.append('positive')\n",
    "    else:\n",
    "        gen_sentiments.append('neutral')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a0e2a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     positive\n",
      "1     positive\n",
      "2     positive\n",
      "3     positive\n",
      "4     positive\n",
      "        ...   \n",
      "95    positive\n",
      "96    positive\n",
      "97     neutral\n",
      "98    negative\n",
      "99    positive\n",
      "Name: gen_sentiments, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['gen_sentiments'] = gen_sentiments\n",
    "print(df['gen_sentiments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df5080",
   "metadata": {},
   "source": [
    "Using VADER sentiment analysis, I based the sentiment for each review based on the compound value which is a normalized value based on the negative, neutral, and positive sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c03cf",
   "metadata": {},
   "source": [
    "## Create Clusters and compute the average, median, high, and low sentiment scores for each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b60670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
    "# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\n",
    "X = vectorizer.fit_transform(df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a56a057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf0d0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize kmeans with 2 centroids\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# fit the model\n",
    "kmeans.fit(X)\n",
    "# store cluster labels in a variable\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b6f3f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f0a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "felt,like,really,story,marvel,characters,bad,good,film,movie\n",
      "\n",
      "Cluster 1\n",
      "far,time,mcu,marvel,like,one,movie,film,spider,man\n"
     ]
    }
   ],
   "source": [
    "def get_top_keywords(n_terms):\n",
    "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
    "    df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
    "    terms = vectorizer.get_feature_names() # access tf-idf terms\n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
    "            \n",
    "get_top_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9fb0de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster0 = []\n",
    "cluster1 = []\n",
    "for i in range(0,100):\n",
    "    if clusters[i] == 0:\n",
    "        cluster0.append(df['cleaned'][i])\n",
    "    else:\n",
    "        cluster1.append(df['cleaned'][i])\n",
    "len(cluster0)\n",
    "len(cluster1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd1a8cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93, 0.9744, 0.979, -0.9586, 0.9891, 0.0096, -0.836, 0.4345, 0.743, 0.7584, 0.3134, 0.9733, -0.6771, -0.9152, 0.8316, 0.765, -0.3687, -0.9901, -0.9538, 0.765, -0.1779, 0.9952, 0.9487, 0.7906, 0.6249, 0.687, 0.9487, 0.9952, 0.9887, 0.9565, 0.9101, 0.872, 0.872, 0.9869, -0.34, 0.9964, 0.9846, 0.9637, 0.9846, 0.9181, 0.891, 0.9274, 0.5994]\n"
     ]
    }
   ],
   "source": [
    "cluster0_sentiments = []\n",
    "for i in cluster0:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster0_sentiments.append(x['compound'])\n",
    "    \n",
    "print(cluster0_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b336c120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentiment for cluster 0 is  0.5137348837209302\n",
      "The median sentiment for cluster 0 is  0.872\n",
      "The low sentiment for cluster 0 is  -0.9901\n",
      "The high sentiment for cluster 0 is  0.9964\n"
     ]
    }
   ],
   "source": [
    "print(\"The average sentiment for cluster 0 is \", np.average(cluster0_sentiments))\n",
    "print(\"The median sentiment for cluster 0 is \", np.median(cluster0_sentiments))\n",
    "print(\"The low sentiment for cluster 0 is \", np.min(cluster0_sentiments))\n",
    "print(\"The high sentiment for cluster 0 is \", np.max(cluster0_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ef35fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9936, 0.9971, 0.9973, 0.9936, 0.9921, 0.9791, -0.0463, 0.4445, 0.9486, 0.9926, 0.9926, 0.5657, 0.9666, 0.9967, 0.9987, 0.7717, 0.9993, 0.9188, -0.591, 0.946, 0.9982, 0.9694, 0.9847, 0.9045, 0.9282, 0.8957, 0.9935, -0.958, 0.7479, 0.765, 0.886, 0.9995, 0.9592, 0.9966, 0.9977, 0.6873, -0.9371, 0.9638, 0.9726, 0.9694, 0.9889, 0.9371, 0.9694, 0.9666, 0.5994, 0.9909, 0.891, 0.9955, 0.5859, 0.8648, 0.9942, -0.505, -0.949, 0.9918, -0.0516, -0.9189, 0.9942]\n"
     ]
    }
   ],
   "source": [
    "cluster1_sentiments = []\n",
    "for i in cluster1:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster1_sentiments.append(x['compound'])\n",
    "    \n",
    "print(cluster1_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5b0192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentiment for cluster 1 is  0.7004666666666667\n",
      "The median sentiment for cluster 1 is  0.9666\n",
      "The low sentiment for cluster 1 is  -0.958\n",
      "The high sentiment for cluster 1 is  0.9995\n"
     ]
    }
   ],
   "source": [
    "print(\"The average sentiment for cluster 1 is \", np.average(cluster1_sentiments))\n",
    "print(\"The median sentiment for cluster 1 is \", np.median(cluster1_sentiments))\n",
    "print(\"The low sentiment for cluster 1 is \", np.min(cluster1_sentiments))\n",
    "print(\"The high sentiment for cluster 1 is \", np.max(cluster1_sentiments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d37cc1",
   "metadata": {},
   "source": [
    "There does not seem to be much difference in the sentiments between the clusters. The averages and medians are close, and both clusters have very low sentiment reviews and very high sentiment reviews. Maybe we will see more separation with more clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a2d137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize kmeans with 2 centroids\n",
    "kmeans = KMeans(n_clusters=9, random_state=42)\n",
    "# fit the model\n",
    "kmeans.fit(X)\n",
    "# store cluster labels in a variable\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "970463f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "years,old,movie,mcu,life,marvel,man,shang,chi,awkwafina\n",
      "\n",
      "Cluster 1\n",
      "great,mcu,last,time,humour,man,far,see,film,spider\n",
      "\n",
      "Cluster 2\n",
      "scarlett,film,much,story,johansson,pugh,florence,movie,black,widow\n",
      "\n",
      "Cluster 3\n",
      "except,say,time,one,much,going,movie,bad,watching,anything\n",
      "\n",
      "Cluster 4\n",
      "movie,forever,film,boseman,chadwick,namor,story,black,wakanda,panther\n",
      "\n",
      "Cluster 5\n",
      "mcu,people,good,one,film,films,ant,movie,man,like\n",
      "\n",
      "Cluster 6\n",
      "fans,really,written,enjoy,excellent,pace,longer,love,fight,sequences\n",
      "\n",
      "Cluster 7\n",
      "know,movie,man,avengers,holland,spider,tom,far,mysterio,peter\n",
      "\n",
      "Cluster 8\n",
      "multiverse,sequel,shows,pacing,hand,version,expect,endless,quickly,spoil\n"
     ]
    }
   ],
   "source": [
    "def get_top_keywords(n_terms):\n",
    "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
    "    df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
    "    terms = vectorizer.get_feature_names() # access tf-idf terms\n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
    "            \n",
    "get_top_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a387eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 5,\n",
       "       3, 3, 5, 6, 5, 5, 2, 1, 0, 5, 8, 3, 5, 3, 5, 5, 1, 8, 5, 7, 7, 7,\n",
       "       4, 0, 1, 5, 0, 0, 5, 0, 6, 5, 7, 0, 0, 3, 6, 5, 0, 4, 5, 2, 2, 0,\n",
       "       4, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 7, 1, 7, 5, 1, 7, 7,\n",
       "       7, 7, 1, 5, 1, 0, 5, 5, 3, 3, 3, 1], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa8967d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "8\n",
      "13\n",
      "9\n",
      "13\n",
      "27\n",
      "3\n",
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "cluster0 = []\n",
    "cluster1 = []\n",
    "cluster2 = []\n",
    "cluster3 = []\n",
    "cluster4 = []\n",
    "cluster5 = []\n",
    "cluster6 = []\n",
    "cluster7 = []\n",
    "cluster8 = []\n",
    "\n",
    "for i in range(0,100):\n",
    "    if clusters[i] == 0:\n",
    "        cluster0.append(df['cleaned'][i])\n",
    "    if clusters[i] == 1:\n",
    "        cluster1.append(df['cleaned'][i])\n",
    "    if clusters[i] == 2:\n",
    "        cluster2.append(df['cleaned'][i])\n",
    "    if clusters[i] == 3:\n",
    "        cluster3.append(df['cleaned'][i])\n",
    "    if clusters[i] == 4:\n",
    "        cluster4.append(df['cleaned'][i])\n",
    "    if clusters[i] == 5:\n",
    "        cluster5.append(df['cleaned'][i])\n",
    "    if clusters[i] == 6:\n",
    "        cluster6.append(df['cleaned'][i])\n",
    "    if clusters[i] == 7:\n",
    "        cluster7.append(df['cleaned'][i])\n",
    "    if clusters[i] == 8:\n",
    "        cluster8.append(df['cleaned'][i])\n",
    "print(len(cluster0))\n",
    "print(len(cluster1))\n",
    "print(len(cluster2))\n",
    "print(len(cluster3))\n",
    "print(len(cluster4))\n",
    "print(len(cluster5))\n",
    "print(len(cluster6))\n",
    "print(len(cluster7))\n",
    "print(len(cluster8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a1a78e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0096, 0.7717, 0.9282, -0.958, 0.7479, 0.765, 0.9995, 0.6249, 0.9887, 0.9592, 0.9966, 0.9977, -0.34, -0.505]\n",
      "[0.8316, 0.946, 0.8957, 0.9371, 0.5994, 0.9274, 0.9942, 0.9942]\n",
      "[-0.9152, 0.872, 0.872, 0.9964, 0.6873, 0.9846, -0.9371, 0.9638, 0.9637, 0.9726, 0.9846, 0.9181, 0.891]\n",
      "[-0.9586, 0.4345, 0.743, 0.9188, -0.3687, 0.687, 0.5994, -0.0516, -0.9189]\n",
      "[-0.0463, 0.4445, 0.9486, 0.9891, 0.9926, 0.9926, 0.5657, 0.9666, 0.9967, 0.9987, 0.9045, 0.9565, 0.9869]\n",
      "[0.9936, 0.93, 0.9971, 0.9973, 0.9744, 0.979, 0.9936, 0.9921, 0.9791, -0.836, 0.7584, 0.9733, -0.6771, 0.9993, -0.591, -0.9901, -0.9538, -0.1779, 0.9935, 0.9952, 0.7906, 0.9952, 0.9101, 0.9666, 0.8648, -0.949, 0.9918]\n",
      "[0.3134, 0.9487, 0.9487]\n",
      "[0.9982, 0.9694, 0.9847, 0.886, 0.9694, 0.9889, 0.9694, 0.9909, 0.891, 0.9955, 0.5859]\n",
      "[0.765, 0.765]\n"
     ]
    }
   ],
   "source": [
    "cluster0_sentiments = []\n",
    "for i in cluster0:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster0_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster0_sentiments)\n",
    "\n",
    "cluster1_sentiments = []\n",
    "for i in cluster1:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster1_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster1_sentiments)\n",
    "\n",
    "cluster2_sentiments = []\n",
    "for i in cluster2:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster2_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster2_sentiments)\n",
    "\n",
    "cluster3_sentiments = []\n",
    "for i in cluster3:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster3_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster3_sentiments)\n",
    "\n",
    "cluster4_sentiments = []\n",
    "for i in cluster4:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster4_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster4_sentiments)\n",
    "\n",
    "\n",
    "cluster5_sentiments = []\n",
    "for i in cluster5:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster5_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster5_sentiments)\n",
    "\n",
    "cluster6_sentiments = []\n",
    "for i in cluster6:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster6_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster6_sentiments)\n",
    "\n",
    "cluster7_sentiments = []\n",
    "for i in cluster7:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster7_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster7_sentiments)\n",
    "\n",
    "cluster8_sentiments = []\n",
    "for i in cluster8:\n",
    "    x = sentiment.polarity_scores(i)\n",
    "    cluster8_sentiments.append(x['compound'])\n",
    "        \n",
    "print(cluster8_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ccabb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_list = [cluster0_sentiments,cluster1_sentiments,cluster2_sentiments,\n",
    "                   cluster3_sentiments,cluster4_sentiments,cluster5_sentiments,\n",
    "                   cluster6_sentiments,cluster7_sentiments,cluster8_sentiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82112139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The averages are  [0.49900000000000005, 0.8907, 0.6349076923076923, 0.12054444444444444, 0.8228230769230771, 0.5148185185185186, 0.7369333333333333, 0.9299363636363637, 0.765]\n",
      "The medians are  [0.7683500000000001, 0.93225, 0.9181, 0.4345, 0.9666, 0.9733, 0.9487, 0.9694, 0.765]\n",
      "The lows are  [-0.958, 0.5994, -0.9371, -0.9586, -0.0463, -0.9901, 0.3134, 0.5859, 0.765]\n",
      "The highs are  [0.9995, 0.9942, 0.9964, 0.9188, 0.9987, 0.9993, 0.9487, 0.9982, 0.765]\n"
     ]
    }
   ],
   "source": [
    "averages = []\n",
    "medians = []\n",
    "lows = []\n",
    "highs = []\n",
    "\n",
    "for i in sentiments_list:\n",
    "    averages.append(np.average(i))\n",
    "    medians.append(np.median(i))\n",
    "    lows.append(np.min(i))\n",
    "    highs.append(np.max(i))\n",
    "    \n",
    "print(\"The averages are \", averages)\n",
    "print(\"The medians are \", medians)\n",
    "print(\"The lows are \", lows)\n",
    "print(\"The highs are \", highs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be58e9f",
   "metadata": {},
   "source": [
    "We see that all of the clusters have positive sentiment on average, though some clusters are significantly lower than others. Clusters 0, 2, 7, and 8 have low average sentiment values. All clusters have very positive sentiment reviews and low sentiment reviews except clusters 1 and 5 which have a low sentiment of 0 and 0.5994, respectively, which are pretty high for minimums. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9a1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
